{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "pd.options.mode.chained_assignment = None\n",
    "from deepfake_detection.defaults import DRIVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KODF_PATH = DRIVE_PATH / \"kodf\"\n",
    "DFDC_PATH = DRIVE_PATH / \"dfdc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_paths(drive_path: Path, file_paths: list[str]) -> list[str]:\n",
    "    bad_paths = []\n",
    "    for path in file_paths:\n",
    "        if not os.path.exists(drive_path / path):\n",
    "            print(f\"Bad path: {drive_path / path}\")\n",
    "            bad_paths.append(path)\n",
    "    \n",
    "    return bad_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proc KODF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepfake_detection.defaults import RANDOM_STATE\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_kodf_train_val_people(unique_persons: list[str], train_size: float = 0.8) -> tuple[list[str], list[str]]:\n",
    "    shuffled_people = shuffle(unique_persons, random_state=RANDOM_STATE)\n",
    "    train_people, val_people = train_test_split(shuffled_people, train_size=train_size)\n",
    "\n",
    "    return train_people, val_people\n",
    "\n",
    "kodf_synthesized_metadata = pd.read_csv(KODF_PATH / \"synthesized_video_metadata.csv\")\n",
    "kodf_synthesized_videos_to_keep = kodf_synthesized_metadata[~kodf_synthesized_metadata.model.isin([\"fo\", \"audio-driven\"])]\n",
    "kodf_synthesized_videos_to_keep['video_path'] = KODF_PATH.relative_to(DRIVE_PATH) / \"synthesized_videos\" / kodf_synthesized_videos_to_keep['model'] / kodf_synthesized_videos_to_keep['date_time'].str.replace(\"-\", \"\") / kodf_synthesized_videos_to_keep['target_person_id'] / kodf_synthesized_videos_to_keep['video']\n",
    "kodf_synthesized_videos_to_keep['video_path'] = kodf_synthesized_videos_to_keep['video_path'].apply(lambda x: str(x))\n",
    "kodf_synthesized_videos_to_keep['label'] = \"FAKE\"\n",
    "kodf_synthesized_videos_to_keep.drop([\"sex\", \"date_time\", \"model\", \"target_person_id\", \"target_video\"], axis=1, inplace=True)\n",
    "kodf_synthesized_videos_to_keep.rename(columns={\"source_person_id\": \"person_id\"}, inplace=True)\n",
    "kodf_synthesized_videos_to_keep.set_index(\"video\", inplace=True)\n",
    "\n",
    "kodf_original_metadata = pd.read_csv(KODF_PATH / \"original_video_metadata.csv\")\n",
    "kodf_original_metadata['video_path'] = KODF_PATH.relative_to(DRIVE_PATH) / \"original_videos\" / kodf_original_metadata[\"person_id\"] / kodf_original_metadata[\"video\"]\n",
    "kodf_original_metadata['video_path'] = kodf_original_metadata['video_path'].apply(lambda x: str(x))\n",
    "kodf_original_metadata['label'] = \"REAL\"\n",
    "kodf_original_metadata.drop([\"sex\", \"date_time\", \"studio\"], axis=1, inplace=True)\n",
    "kodf_original_metadata.set_index(\"video\", inplace=True)\n",
    "\n",
    "kodf_combined = pd.concat([kodf_synthesized_videos_to_keep, kodf_original_metadata])\n",
    "\n",
    "train_people, val_people = get_kodf_train_val_people(kodf_combined['person_id'].unique(), train_size=0.6)\n",
    "val_people, test_people = get_kodf_train_val_people(val_people, train_size=0.5)\n",
    "\n",
    "kodf_combined.loc[kodf_combined['person_id'].isin(train_people), \"split\"] = \"train\"\n",
    "kodf_combined.loc[kodf_combined['person_id'].isin(val_people), \"split\"] = \"val\"\n",
    "kodf_combined.loc[kodf_combined['person_id'].isin(test_people), \"split\"] = \"test\"\n",
    "\n",
    "kodf_combined['dataset'] = \"kodf\"\n",
    "kodf_combined.drop([\"person_id\"], axis=1, inplace=True)\n",
    "\n",
    "bad_paths = get_bad_paths(DRIVE_PATH, kodf_combined['video_path'])\n",
    "    \n",
    "kodf_combined = kodf_combined[~kodf_combined['video_path'].isin(bad_paths)]\n",
    "\n",
    "kodf_combined_dict = kodf_combined.to_dict(orient=\"index\")\n",
    "\n",
    "with open(KODF_PATH / \"kodf_val_train_metadata.json\", 'w') as f:\n",
    "    json.dump(kodf_combined_dict, f)\n",
    "\n",
    "kodf_combined['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proc DFDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for processing \n",
    "\n",
    "parts = [i for i in range(50)]\n",
    "train_metadata = {}\n",
    "for part in parts:\n",
    "    print(f\"Processing Part: {part}\")\n",
    "    folder_name = f\"dfdc_train_part_{str(part).zfill(2)}\"\n",
    "    inner_folder_name = f\"dfdc_train_part_{part}\"\n",
    "    current_part_path = DFDC_PATH / \"train\" / folder_name / inner_folder_name\n",
    "    metadata_path = current_part_path / \"metadata.json\" \n",
    "    part_meta = {}\n",
    "    with open(metadata_path) as f:\n",
    "        meta_data = json.load(f)\n",
    "    for video, data in meta_data.items():\n",
    "        video_path = current_part_path / video\n",
    "        if not os.path.exists(video_path) == True:\n",
    "            print(\"CURRENT VIDEO PATH\", video_path, video, data)\n",
    "            continue\n",
    "        if 'original' in data.keys():\n",
    "            del data['original']\n",
    "\n",
    "        data['video_path'] = str(video_path.relative_to(DRIVE_PATH))\n",
    "        data['dataset'] = \"dfdc\"  \n",
    "        part_meta[video] = data\n",
    "    train_metadata.update(part_meta)\n",
    "\n",
    "with open(DFDC_PATH / \"train_metadata.json\", 'w') as f:\n",
    "    json.dump(train_metadata, f)\n",
    "\n",
    "for split in [\"val\", \"test\"]:\n",
    "    folder_path = DFDC_PATH / split\n",
    "    dfdc_split_df = pd.read_csv(folder_path / \"labels.csv\")\n",
    "    dfdc_split_df.loc[dfdc_split_df['label'] == 1, \"label\"] = \"FAKE\"\n",
    "    dfdc_split_df.loc[dfdc_split_df['label'] == 0, \"label\"] = \"REAL\"\n",
    "    dfdc_split_df.loc[:, \"split\"] = split\n",
    "    dfdc_split_df.loc[:, \"dataset\"] = \"dfdc\"\n",
    "    dfdc_split_df.rename(columns={\"filename\": \"video\"}, inplace=True)\n",
    "    dfdc_split_df.loc[:, \"video_path\"] = folder_path.relative_to(DRIVE_PATH) / dfdc_split_df['video']\n",
    "    dfdc_split_df['video_path'] = dfdc_split_df['video_path'].apply(lambda x: str(x))\n",
    "\n",
    "    bad_paths = get_bad_paths(DRIVE_PATH, dfdc_split_df['video_path'])\n",
    "\n",
    "    dfdc_split_df = dfdc_split_df[~dfdc_split_df['video_path'].isin(bad_paths)]\n",
    "    dfdc_split_df.set_index(\"video\", inplace=True)\n",
    "\n",
    "    dfdc_split_dict = dfdc_split_df.to_dict(orient=\"index\")\n",
    "\n",
    "    with open(DFDC_PATH / f\"{split}_metadata.json\", 'w') as f:\n",
    "        json.dump(dfdc_split_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kodf_df = pd.read_json(KODF_PATH / \"kodf_val_train_metadata.json\", orient=\"index\")\n",
    "dfdc_train_df = pd.read_json(DFDC_PATH / \"train_metadata.json\", orient=\"index\")\n",
    "dfdc_val_df = pd.read_json(DFDC_PATH / \"val_metadata.json\", orient=\"index\")\n",
    "dfdc_test_df = pd.read_json(DFDC_PATH / \"test_metadata.json\", orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.concat([kodf_df, dfdc_train_df, dfdc_val_df, dfdc_test_df])\n",
    "final_dataset.reset_index(inplace=True)\n",
    "final_dataset.columns = [\"video\", \"video_path\", \"label\", \"split\", \"dataset\"]\n",
    "final_dataset.to_csv(DRIVE_PATH / \"metadata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake-detection-GCgYYbau-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
